{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-02-04T16:39:47.478511Z","iopub.status.busy":"2025-02-04T16:39:47.478219Z","iopub.status.idle":"2025-02-04T16:39:47.482371Z","shell.execute_reply":"2025-02-04T16:39:47.481230Z","shell.execute_reply.started":"2025-02-04T16:39:47.478489Z"},"trusted":true},"outputs":[],"source":["# # This Python 3 environment comes with many helpful analytics libraries installed\n","# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# # For example, here's several helpful packages to load\n","\n","# import numpy as np # linear algebra\n","# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# # Input data files are available in the read-only \"../input/\" directory\n","# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","# import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Install InstaGeo"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:43:51.901607Z","iopub.status.busy":"2025-02-05T11:43:51.901270Z","iopub.status.idle":"2025-02-05T11:43:52.042173Z","shell.execute_reply":"2025-02-05T11:43:52.041045Z","shell.execute_reply.started":"2025-02-05T11:43:51.901576Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'InstaGeo-E2E-Geospatial-ML' already exists and is not an empty directory.\n"]}],"source":["## EXECUTE\n","# Clone the InstaGeo-E2E-Geospatial-ML repository from GitHub\n","repository_url = \"https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML\"\n","!git clone {repository_url}"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:43:52.557450Z","iopub.status.busy":"2025-02-05T11:43:52.557098Z","iopub.status.idle":"2025-02-05T11:43:52.865754Z","shell.execute_reply":"2025-02-05T11:43:52.865088Z","shell.execute_reply.started":"2025-02-05T11:43:52.557417Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["From https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML\n","   fb475e7..305cf40  21-demo-new-design-tileserver-integration -> origin/21-demo-new-design-tileserver-integration\n"]},{"name":"stdout","output_type":"stream","text":["Already up to date.\n"]},{"name":"stderr","output_type":"stream","text":["/Users/mayaraayat/Desktop/geoAI/myenv/lib/python3.11/site-packages/IPython/core/completerlib.py:371: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n","  bks = self.db.get('bookmarks',{})\n"]}],"source":["\n","%%bash\n","cd InstaGeo-E2E-Geospatial-ML\n","git pull"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:18:04.906683Z","iopub.status.busy":"2025-02-05T11:18:04.906374Z","iopub.status.idle":"2025-02-05T11:18:46.160987Z","shell.execute_reply":"2025-02-05T11:18:46.160026Z","shell.execute_reply.started":"2025-02-05T11:18:04.906658Z"},"trusted":true},"outputs":[],"source":["\n","%%capture\n","%%bash\n","# Navigate to the cloned InstaGeo-E2E-Geospatial-ML directory\n","cd InstaGeo-E2E-Geospatial-ML\n","# Stash any local changes to avoid conflicts when switching branches\n","git stash\n","#Switch to the 'geo-ai-hack' branch, which likely contains specific code for the Geo AI Hackathon\n","git checkout geo-ai-hack\n","# Install the InstaGeo package \n","pip install -e .[all]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:43:55.230576Z","iopub.status.busy":"2025-02-05T11:43:55.230205Z","iopub.status.idle":"2025-02-05T11:43:55.238452Z","shell.execute_reply":"2025-02-05T11:43:55.237693Z","shell.execute_reply.started":"2025-02-05T11:43:55.230546Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/mayaraayat/Desktop/geoAI/InstaGeo-E2E-Geospatial-ML\n"]},{"name":"stderr","output_type":"stream","text":["/Users/mayaraayat/Desktop/geoAI/myenv/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n","  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"]}],"source":["cd InstaGeo-E2E-Geospatial-ML"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:43:55.520917Z","iopub.status.busy":"2025-02-05T11:43:55.520585Z","iopub.status.idle":"2025-02-05T11:43:55.525777Z","shell.execute_reply":"2025-02-05T11:43:55.524877Z","shell.execute_reply.started":"2025-02-05T11:43:55.520874Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","# Import necessary libraries\n","import os\n","import re\n","import shutil\n","import yaml\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","from pyproj import CRS, Transformer\n","import rasterio\n","os.environ[\"HYDRA_FULL_ERROR\"] =\"1\"\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:15.626166Z","iopub.status.busy":"2025-02-05T11:44:15.625730Z","iopub.status.idle":"2025-02-05T11:44:15.632368Z","shell.execute_reply":"2025-02-05T11:44:15.631480Z","shell.execute_reply.started":"2025-02-05T11:44:15.626122Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","def generate_label_mapping(root_dir, input_subdir, output_csv, start_year=20160101):\n","    \"\"\"\n","    Generate a CSV mapping input chips to corresponding segmentation maps.\n","\n","    Args:\n","        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n","        input_subdir (str): Subdirectory path for chips within the root directory.\n","        output_csv (str or Path): Output path for the generated CSV file.\n","    \"\"\"\n","    root_dir = Path(root_dir)\n","    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n","    \n","    if os.path.exists(root_dir / input_subdir / \"seg_maps\"):\n","        add_label = True\n","    else:\n","        add_label = False\n","\n","    \n","    chips_orig = [\n","        chip for chip in chips_orig\n","        if (chip.startswith(\"chip_\") and int(chip.split(\"_\")[1][:8]) >= start_year)\n","    ]\n","    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n","\n","    if add_label:\n","        seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n","        df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n","    else:\n","        df = pd.DataFrame({\"Input\": chips})\n","    df.to_csv(output_csv, index=False)\n","    \n","    print(f\"Number of rows is: {df.shape[0]}\")\n","    print(f\"CSV generated and saved to: {output_csv}\")\n","\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:15.915628Z","iopub.status.busy":"2025-02-05T11:44:15.915332Z","iopub.status.idle":"2025-02-05T11:44:15.919315Z","shell.execute_reply":"2025-02-05T11:44:15.918578Z","shell.execute_reply.started":"2025-02-05T11:44:15.915604Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","# set data folder path\n","input_dir=\"/kaggle/input/geo-ai-hack\""]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:17.951181Z","iopub.status.busy":"2025-02-05T11:44:17.950786Z","iopub.status.idle":"2025-02-05T11:44:18.170639Z","shell.execute_reply":"2025-02-05T11:44:18.169872Z","shell.execute_reply.started":"2025-02-05T11:44:17.951149Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of rows is: 8411\n","CSV generated and saved to: train_ds.csv\n","Number of rows is: 2404\n","CSV generated and saved to: test_ds.csv\n"]}],"source":["## EXECUTE\n","# Generate label mappings for the training and testing datasets\n","generate_label_mapping(input_dir, 'hls_train/hls_train', \"train_ds.csv\",20180101)\n","generate_label_mapping(input_dir, 'hls_test/hls_test', \"test_ds.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["### Validation Set"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:19.992695Z","iopub.status.busy":"2025-02-05T11:44:19.992385Z","iopub.status.idle":"2025-02-05T11:44:19.997857Z","shell.execute_reply":"2025-02-05T11:44:19.997074Z","shell.execute_reply.started":"2025-02-05T11:44:19.992673Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","def split_validation_data(mapping_csv, validation_split=0.3):\n","    \"\"\"\n","    Split data into training and validation sets based on a CSV file mapping `chips` and `seg_maps`.\n","\n","    Args:\n","        mapping_csv (str or Path): Path to the CSV file containing the mapping between `chips` and `seg_maps`.\n","        data_dir (str or Path): Path to the merged directory containing all files.\n","        validation_dir (str or Path): Path to the new directory for validation files.\n","        validation_split (float): Fraction of the data to use as the validation set.\n","    \"\"\"\n","    df = pd.read_csv(mapping_csv)\n","    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    num_val = int(len(df) * validation_split)\n","    train_df = df[num_val:]\n","    val_df = df[:num_val]\n","    train_df.to_csv(\"train_split.csv\",index=False)    \n","    print(f\"CSV train split  saved to: train_split.csv\")\n","    val_df.to_csv(\"validation_split.csv\",index=False)    \n","    print(f\"CSV validation split  saved to: validation_split.csv\")\n","    \n","    return \n","    "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:20.317325Z","iopub.status.busy":"2025-02-05T11:44:20.316966Z","iopub.status.idle":"2025-02-05T11:44:20.381625Z","shell.execute_reply":"2025-02-05T11:44:20.380570Z","shell.execute_reply.started":"2025-02-05T11:44:20.317300Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CSV train split  saved to: train_split.csv\n","CSV validation split  saved to: validation_split.csv\n"]}],"source":["## EXECUTE\n","# Split the training dataset into training and validation sets\n","split_validation_data(\n","    mapping_csv=\"train_ds.csv\",\n","    validation_split=0.3\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## InstaGeo - Model"]},{"cell_type":"markdown","metadata":{},"source":["After creating our training and validation splits, we can move on to fine-tuning a model that includes a Prithvi backbone paired with a classification head. For regression tasks, the classification head can easily be replaced with a suitable regression head. Additionally, if a completely different model architecture is needed, it can be designed and implemented within this framework."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_yml(filepath):\n","    \"\"\"Load data from a YAML file.\n","\n","    Args:\n","        filepath (str | Path): The path to the YAML file.\n","\n","    Returns:\n","        Dict: The loaded data, or None if the file does not exist.\n","    \"\"\"\n","    filepath=Path(filepath)\n","    with filepath.open() as f:\n","        return yaml.safe_load(f)\n","        \n","def save_yml(data,filepath):\n","    \"\"\"Save data to a YAML file.\n","\n","    Args:\n","        data (Dict): The data to save.\n","        filepath (str | Path): The file path to save the YAML to.\n","    \"\"\"\n","    filepath = Path(filepath)\n","    with filepath.open(\"w\") as f:\n","        yaml.dump(data, f)\n","    print(f\"Data saved to {filepath}.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Launch Training"]},{"cell_type":"markdown","metadata":{},"source":["\n","First, compute the mean and standard deviation for the dataset using the InstaGeo command. Then update the corresponding configuration file [locust.yaml](https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML/blob/main/instageo/model/configs/locust.yaml). In this case, it has already been done for you. However, if you change the dataset split or modify the training data, you should run the command again to compute the new mean and standard deviation."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# %%bash\n","# python -m instageo.model.run --config-name=locust \\\n","#     root_dir=\"/kaggle/input/geo-ai-hack\" \\\n","#     train.batch_size=8 \\\n","#     train.num_epochs=5 \\\n","#     mode=stats \\\n","#     train_filepath=\"train_ds.csv\" \\"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Updat locust file\n","# Load the Locust model configuration file\n","locust_cfg_path=\"InstaGeo-E2E-Geospatial-ML/instageo/model/configs/locust.yaml\"\n","# Load the YAML configuration into a dictionary\n","locust_cfg=load_yml(locust_cfg_path)\n","# Update the mean and standard deviation values in the configuration\n","locust_cfg[\"dataloader\"][\"mean\"]=[670.5441284179688, 1267.7974853515625, 1772.599365234375, 2415.69091796875, 2879.2431640625, 2337.822509765625]\n","locust_cfg[\"dataloader\"][\"std\"]=[2146.305419921875, 2203.416259765625, 2247.03515625, 2310.74755859375, 2322.708984375, 2211.968505859375]\n","# Save the updated configuration back to the YAML file\n","save_yml(locust_cfg,locust_cfg_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train the InstaGeo model using the Locust configuration\n","!python -m instageo.model.run  --config-name=locust \\\n","    hydra.run.dir=\"/kaggle/working/outputs/first_run\" \\\n","    root_dir=\"/kaggle/input/geo-ai-hack\" \\\n","    train.batch_size=12 \\\n","    train.num_epochs=100 \\\n","    mode=train \\\n","    train_filepath=\"train_split.csv\" \\\n","    valid_filepath=\"validation_split.csv\""]},{"cell_type":"markdown","metadata":{},"source":["### Run Model Evaluation\n"," To evaluate the model, adjust the `checkpoint_path` argument to point to the desired model checkpoint. The checkpoint file is typically located in the `hydra.run.dir` directory and is named `instageo_best_checkpoint.ckpt`.\n","For example:\n","```\n","/kaggle/working/outputs/first_run/instageo_best_checkpoint.ckpt \n","```\n","Make sure to provide the correct path to the checkpoint file based on your training output directory."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%bash\n","python -m instageo.model.run --config-name=locust \\\n","    root_dir=\"/kaggle/input/geo-ai-hack\" \\\n","    test_filepath=\"validation_split.csv\" \\\n","    train.batch_size=16 \\\n","    checkpoint_path='/kaggle/working/outputs/first_run/instageo_best_checkpoint.ckpt' \\\n","    mode=eval"]},{"cell_type":"markdown","metadata":{},"source":["## Make Submission"]},{"cell_type":"markdown","metadata":{},"source":["We first run inference on test chips to get the predictions"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:19:01.581142Z","iopub.status.busy":"2025-02-05T11:19:01.580805Z","iopub.status.idle":"2025-02-05T11:24:52.856207Z","shell.execute_reply":"2025-02-05T11:24:52.855439Z","shell.execute_reply.started":"2025-02-05T11:19:01.581115Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[2025-02-05 11:19:22,757][__main__][INFO] - Script: /kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n","[2025-02-05 11:19:22,760][__main__][INFO] - Imported hydra config:\n","root_dir: /kaggle/input/geo-ai-hack\n","valid_filepath: null\n","train_filepath: null\n","test_filepath: test_ds.csv\n","checkpoint_path: /kaggle/input/check/pytorch/default/1/instageo_best_checkpoint.ckpt\n","output_dir: /kaggle/working/predictions\n","mode: chip_inference\n","train:\n","  learning_rate: 0.0001\n","  num_epochs: 20\n","  batch_size: 16\n","  class_weights:\n","  - 1\n","  - 1\n","  ignore_index: -1\n","  weight_decay: 0.1\n","model:\n","  freeze_backbone: false\n","  num_classes: 2\n","dataloader:\n","  bands:\n","  - 0\n","  - 1\n","  - 2\n","  - 3\n","  - 4\n","  - 5\n","  - 6\n","  - 7\n","  - 8\n","  - 9\n","  - 10\n","  - 11\n","  - 12\n","  - 13\n","  - 14\n","  - 15\n","  - 16\n","  - 17\n","  mean:\n","  - 623.2724609375\n","  - 1247.657958984375\n","  - 1772.24169921875\n","  - 2371.256103515625\n","  - 2862.867431640625\n","  - 2357.759765625\n","  std:\n","  - 2182.050048828125\n","  - 2248.420654296875\n","  - 2302.53515625\n","  - 2372.204345703125\n","  - 2398.52685546875\n","  - 2292.96435546875\n","  img_size: 256\n","  temporal_dim: 3\n","  replace_label:\n","  - -9999\n","  - -1\n","  reduce_to_zero: false\n","  no_data_value: -9999\n","  constant_multiplier: 1.0\n","test:\n","  img_size: 256\n","  crop_size: 256\n","  stride: 256\n","  mask_cloud: false\n","\n","[2025-02-05 11:20:28,488][absl][INFO] - Download successful on attempt 1\n","[2025-02-05 11:20:28,682][absl][INFO] - Download successful on attempt 1\n","[2025-02-05 11:20:30,916][root][INFO] - GPU is available. Using GPU...\n"]},{"name":"stderr","output_type":"stream","text":["/kaggle/working/InstaGeo-E2E-Geospatial-ML/instageo/model/model.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(weights_path, map_location=\"cpu\")\n","Running Inference: 100%|██████████| 151/151 [04:19<00:00,  1.72s/it]\n"]}],"source":["%%bash\n","python -m instageo.model.run --config-name=locust \\\n","    root_dir=\"/kaggle/input/geo-ai-hack\" \\\n","    test_filepath=\"test_ds.csv\" \\\n","    train.batch_size=16 \\\n","    checkpoint_path='/kaggle/input/check/pytorch/default/1/instageo_best_checkpoint.ckpt' \\\n","    output_dir='/kaggle/working/predictions' \\\n","    mode=chip_inference"]},{"cell_type":"markdown","metadata":{},"source":["After getting the prdictions for each chip, we retrieve the predicted value for each observatio in our test split."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:25:01.240470Z","iopub.status.busy":"2025-02-05T11:25:01.240163Z","iopub.status.idle":"2025-02-05T11:25:01.248464Z","shell.execute_reply":"2025-02-05T11:25:01.247599Z","shell.execute_reply.started":"2025-02-05T11:25:01.240444Z"},"trusted":true},"outputs":[],"source":["\n","\n","predictions_directory = \"/kaggle/working/predictions\"\n","prediction_files = os.listdir(predictions_directory)\n","\n","def get_prediction_value(row):\n","    matching_files = [f for f in prediction_files if (str(row['date']) in f) and (row['mgrs_tile_id'] in f)]\n","    if not matching_files:\n","        return (np.nan, np.nan)\n","    for file in matching_files:\n","        with rasterio.open(f\"{predictions_directory}/{file}\") as src:\n","            width, height = src.width, src.height\n","            affine_transform = rasterio.transform.AffineTransformer(src.transform)\n","            transformer = Transformer.from_crs(CRS.from_epsg(4326), src.crs, always_xy=True)\n","            x_chip, y_chip = transformer.transform(row['x'], row['y'])\n","            x_offset, y_offset = affine_transform.rowcol(x_chip, y_chip)\n","            \n","            if 0 <= x_offset < width and 0 <= y_offset < height:\n","                return src.read(1)[y_offset, x_offset], file\n","    return (np.nan, np.nan)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:25:05.702700Z","iopub.status.busy":"2025-02-05T11:25:05.702392Z","iopub.status.idle":"2025-02-05T11:33:18.095936Z","shell.execute_reply":"2025-02-05T11:33:18.095040Z","shell.execute_reply.started":"2025-02-05T11:25:05.702673Z"},"trusted":true},"outputs":[],"source":["submission_df = pd.read_csv(\"/kaggle/input/geo-ai-hack/test.csv\")\n","\n","submission_df[['prediction', 'filename']] = submission_df.apply(get_prediction_value, axis=1, result_type='expand')\n","submission_df[[\"id\",\"prediction\"]].to_csv(\"hls_submission.csv\",index=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:00.730723Z","iopub.status.busy":"2025-02-05T11:44:00.730327Z","iopub.status.idle":"2025-02-05T11:44:00.761161Z","shell.execute_reply":"2025-02-05T11:44:00.760416Z","shell.execute_reply.started":"2025-02-05T11:44:00.730689Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","\n","# Code inspired and modified from the InstaGeo-E2E-Geospatial-ML repository\n","\n","\"\"\"Dataloader Module.\"\"\"\n","\n","import os\n","import random\n","from functools import partial\n","from typing import Callable, List, Tuple\n","\n","import numpy as np\n","import sys\n","import pandas as pd\n","import rasterio\n","import torch\n","from absl import logging\n","from PIL import Image\n","from torchvision import transforms\n","import rioxarray as rxr\n","import xarray as xr\n","\n","def open_mf_tiff_dataset(\n","    band_files: dict, load_masks: bool\n",") -> tuple[xr.Dataset, xr.Dataset | None, CRS]:\n","    \"\"\"Open multiple TIFF files as an xarray Dataset.\n","\n","    Args:\n","        band_files (Dict[str, Dict[str, str]]): A dictionary mapping band names to file paths.\n","        load_masks (bool): Whether or not to load the masks files.\n","\n","    Returns:\n","        (xr.Dataset, xr.Dataset | None, CRS): A tuple of xarray Dataset combining data from all the\n","            provided TIFF files, (optionally) the masks, and the CRS\n","    \"\"\"\n","    band_paths = list(band_files[\"tiles\"].values())\n","    bands_dataset = xr.open_mfdataset(\n","        band_paths,\n","        concat_dim=\"band\",\n","        combine=\"nested\",\n","        mask_and_scale=False,  # Scaling will be applied manually\n","    )\n","    bands_dataset.band_data.attrs[\"scale_factor\"] = 1\n","    mask_paths = list(band_files[\"fmasks\"].values())\n","    mask_dataset = (\n","        xr.open_mfdataset(\n","            mask_paths,\n","            concat_dim=\"band\",\n","            combine=\"nested\",\n","        )\n","        if load_masks\n","        else None\n","    )\n","    with rasterio.open(band_paths[0]) as src:\n","        crs = src.crs\n","    return bands_dataset, mask_dataset, crs\n","\n","\n","def random_crop_and_flip(\n","    ims: List[Image.Image], label: Image.Image, im_size: int\n",") -> Tuple[List[Image.Image], Image.Image]:\n","    \"\"\"Apply random cropping and flipping transformations to the given images and label.\n","\n","    Args:\n","        ims (List[Image.Image]): List of PIL Image objects representing the images.\n","        label (Image.Image): A PIL Image object representing the label.\n","\n","    Returns:\n","        Tuple[List[Image.Image], Image.Image]: A tuple containing the transformed list of\n","        images and label.\n","    \"\"\"\n","    i, j, h, w = transforms.RandomCrop.get_params(ims[0], (im_size, im_size))\n","\n","    ims = [transforms.functional.crop(im, i, j, h, w) for im in ims]\n","    label = transforms.functional.crop(label, i, j, h, w)\n","\n","    if random.random() > 0.5:\n","        ims = [transforms.functional.hflip(im) for im in ims]\n","        label = transforms.functional.hflip(label)\n","\n","    if random.random() > 0.5:\n","        ims = [transforms.functional.vflip(im) for im in ims]\n","        label = transforms.functional.vflip(label)\n","\n","    return ims, label\n","\n","\n","def normalize_and_convert_to_tensor(\n","    ims: List[Image.Image],\n","    label: Image.Image | None,\n","    mean: List[float],\n","    std: List[float],\n","    temporal_size: int = 1,\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"Normalize the images and label and convert them to PyTorch tensors.\n","\n","    Args:\n","        ims (List[Image.Image]): List of PIL Image objects representing the images.\n","        label (Image.Image | None): A PIL Image object representing the label.\n","        mean (List[float]): The mean of each channel in the image\n","        std (List[float]): The standard deviation of each channel in the image\n","        temporal_size: The number of temporal steps\n","\n","    Returns:\n","        Tuple[torch.Tensor, torch.Tensor]: A tuple of tensors representing the normalized\n","        images and label.\n","    \"\"\"\n","    norm = transforms.Normalize(mean, std)\n","    ims_tensor = torch.stack([transforms.ToTensor()(im).squeeze() for im in ims])\n","    _, h, w = ims_tensor.shape\n","    ims_tensor = ims_tensor.reshape([temporal_size, -1, h, w])  # T*C,H,W -> T,C,H,W\n","    ims_tensor = torch.stack([norm(im) for im in ims_tensor]).permute(\n","        [1, 0, 2, 3]\n","    )  # T,C,H,W -> C,T,H,W\n","    if label:\n","        label = torch.from_numpy(np.array(label)).squeeze()\n","    return ims_tensor, label\n","\n","\n","def process_and_augment(\n","    x: np.ndarray,\n","    y: np.ndarray | None,\n","    mean: List[float],\n","    std: List[float],\n","    temporal_size: int = 1,\n","    im_size: int = 224,\n","    augment: bool = True,\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"Process and augment the given images and labels.\n","\n","    Args:\n","        x (np.ndarray): Numpy array representing the images.\n","        y (np.ndarray): Numpy array representing the label.\n","        mean (List[float]): The mean of each channel in the image\n","        std (List[float]): The standard deviation of each channel in the image\n","        temporal_size: The number of temporal steps\n","        augment: Flag to perform augmentations in training mode.\n","\n","    Returns:\n","        Tuple[torch.Tensor, torch.Tensor]: A tuple of tensors representing the processed\n","        and augmented images and label.\n","    \"\"\"\n","    ims = x.copy()\n","    label = None\n","    # convert to PIL for easier transforms\n","    ims = [Image.fromarray(im) for im in ims]\n","    if y is not None:\n","        label = Image.fromarray(y.copy().squeeze())\n","    if augment:\n","        ims, label = random_crop_and_flip(ims, label, im_size)\n","    ims, label = normalize_and_convert_to_tensor(ims, label, mean, std, temporal_size)\n","    return ims, label\n","\n","\n","def crop_array(\n","    arr: np.ndarray, left: int, top: int, right: int, bottom: int\n",") -> np.ndarray:\n","    \"\"\"Crop Numpy Image.\n","\n","    Crop a given array (image) using specified left, top, right, and bottom indices.\n","\n","    This function supports cropping both grayscale (2D) and color (3D) images.\n","\n","    Args:\n","        arr (np.ndarray): The input array (image) to be cropped.\n","        left (int): The left boundary index for cropping.\n","        top (int): The top boundary index for cropping.\n","        right (int): The right boundary index for cropping.\n","        bottom (int): The bottom boundary index for cropping.\n","\n","    Returns:\n","        np.ndarray: The cropped portion of the input array (image).\n","\n","    Raises:\n","        ValueError: If the input array is not 2D or 3D.\n","    \"\"\"\n","    if len(arr.shape) == 2:  # Grayscale image (2D array)\n","        return arr[top:bottom, left:right]\n","    elif len(arr.shape) == 3:  # Color image (3D array)\n","        return arr[:, top:bottom, left:right]\n","    elif len(arr.shape) == 4:  # Color image (3D array)\n","        return arr[:, :, top:bottom, left:right]\n","    else:\n","        raise ValueError(\"Input array must be a 2D, 3D or 4D array\")\n","\n","\n","\n","def get_raster_data(\n","    fname: str | dict[str, dict[str, str]],\n","    is_label: bool = True,\n","    bands: List[int] | None = None,\n","    no_data_value: int | None = -9999,\n","    mask_cloud: bool = True,\n","    water_mask: bool = False,\n",") -> np.ndarray:\n","    \"\"\"Load and process raster data from a file.\n","\n","    Args:\n","        fname (str): Filename to load data from.\n","        is_label (bool): Whether the file is a label file.\n","        bands (List[int]): Index of bands to select from array.\n","        no_data_value (int | None): NODATA value in image raster.\n","        mask_cloud (bool): Perform cloud masking.\n","        water_mask (bool): Perform water masking.\n","\n","    Returns:\n","        np.ndarray: Numpy array representing the processed data.\n","    \"\"\"\n","    if isinstance(fname, dict):\n","        data, mask, crs = open_mf_tiff_dataset(fname, load_masks=False)\n","        data = data.fillna(no_data_value)\n","        data = data.band_data.values\n","    else:\n","        with rasterio.open(fname) as src:\n","            data = src.read()\n","    if (not is_label) and bands:\n","        data = data[bands, ...]\n","    # For some reasons, some few HLS tiles are not scaled in v2.0.\n","    # In the following lines, we find and scale them\n","    bands = []\n","    for band in data:\n","        band = band.astype(np.float32)\n","        if band.max() > 10:\n","            band *= 0.0001\n","        bands.append(band)\n","    data = np.stack(bands, axis=0)\n","    \n","    return data\n","from typing import List, Tuple\n","import numpy as np\n","import torch\n","from functools import partial\n","\n","def process_test(\n","    x: np.ndarray,\n","    mean: List[float],\n","    std: List[float],\n","    temporal_size: int = 1,\n","    img_size: int = 512,\n","    crop_size: int = 224,\n","    stride: int = 224,\n",") -> torch.Tensor:\n","    \"\"\"Process and augment test data without expecting masks.\n","\n","    Args:\n","        x (np.ndarray): Input image array.\n","        mean (List[float]): Mean values for normalization.\n","        std: (List[float]): Standard deviation values for normalization.\n","        temporal_size (int, optional): Temporal dimension size. Defaults to 1.\n","        img_size (int, optional): Size of the input images. Defaults to 512.\n","        crop_size (int, optional): Size of the crops to be extracted from the images. Defaults to 224.\n","        stride (int, optional): Stride for cropping. Defaults to 224.\n","\n","    Returns:\n","        torch.Tensor: Tensor containing the processed images.\n","    \"\"\"\n","    preprocess_func = partial(\n","        process_and_augment,\n","        mean=mean,\n","        std=std,\n","        temporal_size=temporal_size,\n","        augment=False,\n","    )\n","\n","    img_crops = []\n","    width, height = img_size, img_size\n","\n","    for top in range(0, height - crop_size + 1, stride):\n","        for left in range(0, width - crop_size + 1, stride):\n","            bottom = top + crop_size\n","            right = left + crop_size\n","\n","            img_crops.append(crop_array(x, left, top, right, bottom))\n","\n","    # Process images using the preprocessing function\n","    samples = [preprocess_func(img, None) for img in img_crops]\n","    imgs = torch.stack([sample[0] for sample in samples])\n","    return imgs\n","\n","\n","def process_data(\n","    im_fname: str,\n","    mask_fname: str | None = None,\n","    no_data_value: int | None = -9999,\n","    reduce_to_zero: bool = False,\n","    replace_label: Tuple | None = None,\n","    bands: List[int] | None = None,\n","    constant_multiplier: float = 1.0,\n","    mask_cloud: bool = False,\n",") -> Tuple[np.ndarray, np.ndarray]:\n","    \"\"\"Process image and mask data from filenames.\n","\n","    Args:\n","        im_fname (str): Filename for the image data.\n","        mask_fname (str | None): Filename for the mask data.\n","        bands (List[int]): Indices of bands to select from array.\n","        no_data_value (int | None): NODATA value in image raster.\n","        reduce_to_zero (bool): Reduces the label index to start from Zero.\n","        replace_label (Tuple): Tuple of value to replace and the replacement value.\n","        constant_multiplier (float): Constant multiplier for image.\n","        mask_cloud (bool): Perform cloud masking.\n","\n","    Returns:\n","        Tuple[np.ndarray, np.ndarray]: A tuple of numpy arrays representing the processed\n","        image and mask data.\n","    \"\"\"\n","    arr_x = get_raster_data(\n","        im_fname,\n","        is_label=False,\n","        bands=bands,\n","        no_data_value=no_data_value,\n","        mask_cloud=mask_cloud,\n","        water_mask=False,\n","    )\n","    arr_x = arr_x * constant_multiplier\n","    if mask_fname:\n","        arr_y = get_raster_data(mask_fname)\n","        if replace_label:\n","            arr_y = np.where(arr_y == replace_label[0], replace_label[1], arr_y)\n","        if reduce_to_zero:\n","            arr_y -= 1\n","    else:\n","        arr_y = None\n","    return arr_x, arr_y\n","\n","\n","def load_data_from_csv(fname: str, input_root: str) -> List[Tuple[str, str | None]]:\n","    \"\"\"Load data file paths from a CSV file.\n","\n","    Args:\n","        fname (str): Filename of the CSV file.\n","        input_root (str): Root directory for input images and labels.\n","\n","    Returns:\n","        List[Tuple[str, str]]: A list of tuples, each containing file paths for input\n","        image and label image.\n","    \"\"\"\n","    file_paths = []\n","    data = pd.read_csv(fname)\n","    label_present = True if \"Label\" in data.columns else False\n","    for _, row in data.iterrows():\n","        im_path = os.path.join(input_root, row[\"Input\"])\n","        mask_path = (\n","            None if not label_present else os.path.join(input_root, row[\"Label\"])\n","        )\n","        if os.path.exists(im_path):\n","            try:\n","                with rasterio.open(im_path) as src:\n","                    _ = src.crs\n","                file_paths.append((im_path, mask_path))\n","            except Exception as e:\n","                logging.error(e)\n","                continue\n","    return file_paths\n","\n","\n","class InstaGeoDataset(torch.utils.data.Dataset):\n","    \"\"\"InstaGeo PyTorch Dataset for Loading and Handling HLS Data.\"\"\"\n","\n","    def __init__(\n","        self,\n","        filename: str,\n","        input_root: str,\n","        preprocess_func: Callable,\n","        no_data_value: int | None,\n","        replace_label: Tuple,\n","        reduce_to_zero: bool,\n","        constant_multiplier: float,\n","        bands: List[int] | None = None,\n","        include_filenames: bool = False,\n","    ):\n","        \"\"\"Dataset Class for loading and preprocessing the dataset.\n","\n","        Args:\n","            filename (str): Filename of the CSV file containing data paths.\n","            input_root (str): Root directory for input images and labels.\n","            preprocess_func (Callable): Function to preprocess the data.\n","            bands (List[int]): Indices of bands to select from array.\n","            no_data_value (int | None): NODATA value in image raster.\n","            reduce_to_zero (bool): Reduces the label index to start from Zero.\n","            replace_label (Tuple): Tuple of value to replace and the replacement value.\n","            constant_multiplier (float): Constant multiplier for image.\n","            include_filenames (bool): Flag that determines whether to return filenames.\n","\n","        \"\"\"\n","        self.input_root = input_root\n","        self.preprocess_func = preprocess_func\n","        self.bands = bands\n","        self.file_paths = load_data_from_csv(filename, input_root)\n","        self.no_data_value = no_data_value\n","        self.replace_label = replace_label\n","        self.reduce_to_zero = reduce_to_zero\n","        self.constant_multiplier = constant_multiplier\n","        self.include_filenames = include_filenames\n","\n","    def __getitem__(self, i: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","        \"\"\"Retrieves a sample from dataset.\n","\n","        Args:\n","            i (int): Sample index to retrieve.\n","\n","        Returns:\n","            Tuple[torch.Tensor, torch.Tensor]: A tuple of tensors representing the\n","            processed images and label.\n","        \"\"\"\n","        im_fname, mask_fname = self.file_paths[i]\n","        arr_x, arr_y = process_data(\n","            im_fname,\n","            mask_fname,\n","            no_data_value=self.no_data_value,\n","            replace_label=self.replace_label,\n","            reduce_to_zero=self.reduce_to_zero,\n","            bands=self.bands,\n","            constant_multiplier=self.constant_multiplier,\n","        )\n","        img_tensor, mask_tensor = self.preprocess_func(arr_x, arr_y)\n","\n","        return {\"image\": img_tensor, \"mask\": mask_tensor.long()}  # 🔥 Return a dictionary\n","\n","    def __len__(self) -> int:\n","        \"\"\"Return length of dataset.\"\"\"\n","        return len(self.file_paths)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:03.275469Z","iopub.status.busy":"2025-02-05T11:44:03.275050Z","iopub.status.idle":"2025-02-05T11:44:03.285136Z","shell.execute_reply":"2025-02-05T11:44:03.284223Z","shell.execute_reply.started":"2025-02-05T11:44:03.275437Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","dict_config = {\n","    \"root_dir\": None,\n","    \"valid_filepath\": None,\n","    \"train_filepath\": None,\n","    \"test_filepath\": None,\n","    \"checkpoint_path\": None,\n","    \"mode\": \"train\",  # \"train\" or \"eval\"\n","\n","    \"train\": {\n","        \"learning_rate\": 0.0001,\n","        \"num_epochs\": 20,\n","        \"batch_size\": 16,\n","        \"class_weights\": [1, 1],\n","        \"ignore_index\": -1,\n","        \"weight_decay\": 0.1\n","    },\n","\n","    \"model\": {\n","        \"freeze_backbone\": True,  # Set to False if required\n","        \"num_classes\": 2\n","    },\n","\n","    \"dataloader\": {\n","        \"bands\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17],\n","        \"mean\": [670.5441284179688, 1267.7974853515625, 1772.599365234375, 2415.69091796875, 2879.2431640625, 2337.822509765625],\n","        \"std\": [2146.305419921875, 2203.416259765625, 2247.03515625, 2310.74755859375, 2322.708984375, 2211.968505859375],\n","        \"img_size\": 256,\n","        \"temporal_dim\": 3,\n","        \"replace_label\": [-9999, -1],\n","        \"reduce_to_zero\": False,\n","        \"no_data_value\": -9999,\n","        \"constant_multiplier\": 1.0\n","    },\n","\n","    \"test\": {\n","        \"img_size\": 256,\n","        \"crop_size\": 256,\n","        \"stride\": 256,\n","        \"mask_cloud\": False\n","    }\n","}\n","\n","# Assigning values from the dictionary\n","BANDS = dict_config[\"dataloader\"][\"bands\"]\n","MEAN = dict_config[\"dataloader\"][\"mean\"]\n","STD = dict_config[\"dataloader\"][\"std\"]\n","IM_SIZE = dict_config[\"dataloader\"][\"img_size\"]\n","TEMPORAL_SIZE = dict_config[\"dataloader\"][\"temporal_dim\"]\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:27.906139Z","iopub.status.busy":"2025-02-05T11:44:27.905764Z","iopub.status.idle":"2025-02-05T11:44:27.935071Z","shell.execute_reply":"2025-02-05T11:44:27.934298Z","shell.execute_reply.started":"2025-02-05T11:44:27.906111Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Input</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hls_train/hls_train/chips/chip_20180401_S30_T4...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20180401_...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hls_train/hls_train/chips/chip_20180601_S30_T4...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20180601_...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hls_train/hls_train/chips/chip_20200701_S30_T4...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20200701_...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>hls_train/hls_train/chips/chip_20190101_S30_T3...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20190101_...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hls_train/hls_train/chips/chip_20181101_S30_T2...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20181101_...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5883</th>\n","      <td>hls_train/hls_train/chips/chip_20191101_S30_T4...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20191101_...</td>\n","    </tr>\n","    <tr>\n","      <th>5884</th>\n","      <td>hls_train/hls_train/chips/chip_20200701_S30_T4...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20200701_...</td>\n","    </tr>\n","    <tr>\n","      <th>5885</th>\n","      <td>hls_train/hls_train/chips/chip_20180601_S30_T4...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20180601_...</td>\n","    </tr>\n","    <tr>\n","      <th>5886</th>\n","      <td>hls_train/hls_train/chips/chip_20191201_S30_T3...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20191201_...</td>\n","    </tr>\n","    <tr>\n","      <th>5887</th>\n","      <td>hls_train/hls_train/chips/chip_20200201_L30_T3...</td>\n","      <td>hls_train/hls_train/seg_maps/seg_map_20200201_...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5888 rows × 2 columns</p>\n","</div>"],"text/plain":["                                                  Input  \\\n","0     hls_train/hls_train/chips/chip_20180401_S30_T4...   \n","1     hls_train/hls_train/chips/chip_20180601_S30_T4...   \n","2     hls_train/hls_train/chips/chip_20200701_S30_T4...   \n","3     hls_train/hls_train/chips/chip_20190101_S30_T3...   \n","4     hls_train/hls_train/chips/chip_20181101_S30_T2...   \n","...                                                 ...   \n","5883  hls_train/hls_train/chips/chip_20191101_S30_T4...   \n","5884  hls_train/hls_train/chips/chip_20200701_S30_T4...   \n","5885  hls_train/hls_train/chips/chip_20180601_S30_T4...   \n","5886  hls_train/hls_train/chips/chip_20191201_S30_T3...   \n","5887  hls_train/hls_train/chips/chip_20200201_L30_T3...   \n","\n","                                                  Label  \n","0     hls_train/hls_train/seg_maps/seg_map_20180401_...  \n","1     hls_train/hls_train/seg_maps/seg_map_20180601_...  \n","2     hls_train/hls_train/seg_maps/seg_map_20200701_...  \n","3     hls_train/hls_train/seg_maps/seg_map_20190101_...  \n","4     hls_train/hls_train/seg_maps/seg_map_20181101_...  \n","...                                                 ...  \n","5883  hls_train/hls_train/seg_maps/seg_map_20191101_...  \n","5884  hls_train/hls_train/seg_maps/seg_map_20200701_...  \n","5885  hls_train/hls_train/seg_maps/seg_map_20180601_...  \n","5886  hls_train/hls_train/seg_maps/seg_map_20191201_...  \n","5887  hls_train/hls_train/seg_maps/seg_map_20200201_...  \n","\n","[5888 rows x 2 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["pd.read_csv(\"train_split.csv\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:28.665964Z","iopub.status.busy":"2025-02-05T11:44:28.665624Z","iopub.status.idle":"2025-02-05T11:44:28.678605Z","shell.execute_reply":"2025-02-05T11:44:28.677834Z","shell.execute_reply.started":"2025-02-05T11:44:28.665939Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Input</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hls_test/hls_test/chips/chip_20210101_S30_T42Q...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>hls_test/hls_test/chips/chip_20210501_S30_T42Q...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hls_test/hls_test/chips/chip_20210301_L30_T43R...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>hls_test/hls_test/chips/chip_20211201_L30_T36Q...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hls_test/hls_test/chips/chip_20210101_S30_T37Q...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2399</th>\n","      <td>hls_test/hls_test/chips/chip_20210101_S30_T38P...</td>\n","    </tr>\n","    <tr>\n","      <th>2400</th>\n","      <td>hls_test/hls_test/chips/chip_20211201_S30_T37Q...</td>\n","    </tr>\n","    <tr>\n","      <th>2401</th>\n","      <td>hls_test/hls_test/chips/chip_20210701_S30_T39Q...</td>\n","    </tr>\n","    <tr>\n","      <th>2402</th>\n","      <td>hls_test/hls_test/chips/chip_20210501_S30_T40Q...</td>\n","    </tr>\n","    <tr>\n","      <th>2403</th>\n","      <td>hls_test/hls_test/chips/chip_20210401_S30_T43R...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2404 rows × 1 columns</p>\n","</div>"],"text/plain":["                                                  Input\n","0     hls_test/hls_test/chips/chip_20210101_S30_T42Q...\n","1     hls_test/hls_test/chips/chip_20210501_S30_T42Q...\n","2     hls_test/hls_test/chips/chip_20210301_L30_T43R...\n","3     hls_test/hls_test/chips/chip_20211201_L30_T36Q...\n","4     hls_test/hls_test/chips/chip_20210101_S30_T37Q...\n","...                                                 ...\n","2399  hls_test/hls_test/chips/chip_20210101_S30_T38P...\n","2400  hls_test/hls_test/chips/chip_20211201_S30_T37Q...\n","2401  hls_test/hls_test/chips/chip_20210701_S30_T39Q...\n","2402  hls_test/hls_test/chips/chip_20210501_S30_T40Q...\n","2403  hls_test/hls_test/chips/chip_20210401_S30_T43R...\n","\n","[2404 rows x 1 columns]"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["pd.read_csv('test_ds.csv')"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:44:30.670871Z","iopub.status.busy":"2025-02-05T11:44:30.670533Z","iopub.status.idle":"2025-02-05T11:45:17.279425Z","shell.execute_reply":"2025-02-05T11:45:17.278609Z","shell.execute_reply.started":"2025-02-05T11:44:30.670843Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","train_filepath = \"train_split.csv\"\n","val_filepath = \"validation_split.csv\"\n","root_dir = \"/kaggle/input/geo-ai-hack\"\n","train_dataset = InstaGeoDataset(\n","            filename=train_filepath,\n","            input_root=root_dir,\n","            preprocess_func=partial(\n","                process_and_augment,\n","                mean=MEAN,\n","                std=STD,\n","                temporal_size=TEMPORAL_SIZE,\n","                im_size=IM_SIZE,\n","            ),\n","            bands=BANDS,\n","            replace_label=dict_config[\"dataloader\"]['replace_label'],\n","            reduce_to_zero=dict_config[\"dataloader\"]['reduce_to_zero'],\n","            no_data_value=dict_config[\"dataloader\"]['no_data_value'],\n","            constant_multiplier=dict_config[\"dataloader\"]['constant_multiplier'],\n","        )\n","val_dataset = InstaGeoDataset(\n","            filename=val_filepath,\n","            input_root=root_dir,\n","            preprocess_func=partial(\n","                process_and_augment,\n","                mean=MEAN,\n","                std=STD,\n","                temporal_size=TEMPORAL_SIZE,\n","                im_size=IM_SIZE,\n","            ),\n","            bands=BANDS,\n","            replace_label=dict_config[\"dataloader\"]['replace_label'],\n","            reduce_to_zero=dict_config[\"dataloader\"]['reduce_to_zero'],\n","            no_data_value=dict_config[\"dataloader\"]['no_data_value'],\n","            constant_multiplier=dict_config[\"dataloader\"]['constant_multiplier'],\n","        )"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:45:17.280689Z","iopub.status.busy":"2025-02-05T11:45:17.280437Z","iopub.status.idle":"2025-02-05T11:45:17.285640Z","shell.execute_reply":"2025-02-05T11:45:17.284760Z","shell.execute_reply.started":"2025-02-05T11:45:17.280666Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","from torch.utils.data import DataLoader, Dataset\n","from typing import Optional\n","def create_dataloader(\n","    dataset: Dataset,\n","    batch_size: int,\n","    shuffle: bool = False,\n","    num_workers: int = 1,\n","    collate_fn: Optional[Callable] = None,\n","    pin_memory: bool = True,\n",") -> DataLoader:\n","    \"\"\"Create a DataLoader for the given dataset.\n","\n","    This function is a convenient wrapper around the PyTorch DataLoader class,\n","    allowing for easy setup of various DataLoader parameters.\n","\n","    Args:\n","        dataset (Dataset): The dataset to load data from.\n","        batch_size (int): How many samples per batch to load.\n","        shuffle (bool): Set to True to have the data reshuffled at every epoch.\n","        num_workers (int): How many subprocesses to use for data loading.\n","        collate_fn (Optional[Callable]): Merges a list of samples to form a mini-batch.\n","        pin_memory (bool): If True, the data loader will copy tensors into CUDA pinned\n","            memory.\n","\n","    Returns:\n","        DataLoader: An instance of the PyTorch DataLoader.\n","    \"\"\"\n","    return DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        num_workers=num_workers,\n","        collate_fn=collate_fn,\n","        pin_memory=pin_memory,\n","    )\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:45:17.287435Z","iopub.status.busy":"2025-02-05T11:45:17.287182Z","iopub.status.idle":"2025-02-05T11:45:17.307515Z","shell.execute_reply":"2025-02-05T11:45:17.306893Z","shell.execute_reply.started":"2025-02-05T11:45:17.287413Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","batch_size = dict_config['train']['batch_size']\n","train_loader = create_dataloader(\n","            train_dataset, batch_size=batch_size, shuffle=True, num_workers=1\n","        )\n","val_loader = create_dataloader(\n","            val_dataset, batch_size=batch_size, shuffle=True, num_workers=1\n","        )"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:45:17.308729Z","iopub.status.busy":"2025-02-05T11:45:17.308441Z","iopub.status.idle":"2025-02-05T11:45:20.560042Z","shell.execute_reply":"2025-02-05T11:45:20.558965Z","shell.execute_reply.started":"2025-02-05T11:45:17.308693Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch Shape: torch.Size([16, 6, 3, 256, 256])\n","Label Shape: torch.Size([16, 256, 256])\n","368\n"]}],"source":["## EXECUTE\n","for batch in train_loader:\n","\n","    print(f\"Batch Shape: {batch['image'].shape}\")\n","    print(f\"Label Shape: {batch['mask'].shape}\")\n","    break\n","print(len(train_loader))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:45:20.561365Z","iopub.status.busy":"2025-02-05T11:45:20.561040Z","iopub.status.idle":"2025-02-05T11:45:23.806150Z","shell.execute_reply":"2025-02-05T11:45:23.805227Z","shell.execute_reply.started":"2025-02-05T11:45:20.561335Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Batch Shape: torch.Size([16, 6, 3, 256, 256])\n","Label Shape: torch.Size([16, 256, 256])\n","158\n"]}],"source":["## EXECUTE\n","for batch in val_loader:\n","    print(f\"Batch Shape: {batch['image'].shape}\")\n","    print(f\"Label Shape: {batch['mask'].shape}\")\n","    break\n","print(len(val_loader))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:37:37.258429Z","iopub.status.busy":"2025-02-05T11:37:37.258099Z","iopub.status.idle":"2025-02-05T11:41:08.315943Z","shell.execute_reply":"2025-02-05T11:41:08.314948Z","shell.execute_reply.started":"2025-02-05T11:37:37.258399Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting terratorch\n","  Downloading terratorch-0.99.7-py3-none-any.whl.metadata (5.3 kB)\n","Collecting torch<=2.5.0,>=2.1.0 (from terratorch)\n","  Downloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n","Requirement already satisfied: torchvision>=0.16.0 in /usr/local/lib/python3.10/dist-packages (from terratorch) (0.20.1+cu121)\n","Collecting torchgeo>=0.6.0 (from terratorch)\n","  Downloading torchgeo-0.6.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: rioxarray>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from terratorch) (0.18.2)\n","Collecting albumentations<=1.4.10,>=1.3.1 (from terratorch)\n","  Downloading albumentations-1.4.10-py3-none-any.whl.metadata (38 kB)\n","Collecting albucore<=0.0.16 (from terratorch)\n","  Downloading albucore-0.0.16-py3-none-any.whl.metadata (3.1 kB)\n","Requirement already satisfied: rasterio>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from terratorch) (1.4.3)\n","Collecting torchmetrics<=1.3.1 (from terratorch)\n","  Downloading torchmetrics-1.3.1-py3-none-any.whl.metadata (19 kB)\n","Collecting geopandas>=0.14.4 (from terratorch)\n","  Downloading geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n","Collecting lightly>=1.4.25 (from terratorch)\n","  Downloading lightly-1.5.18-py3-none-any.whl.metadata (36 kB)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from terratorch) (3.12.1)\n","Collecting mlflow>=2.12.1 (from terratorch)\n","  Downloading mlflow-2.20.1-py3-none-any.whl.metadata (30 kB)\n","Collecting lightning!=2.3.*,>=2 (from lightning[pytorch-extra]!=2.3.*,>=2->terratorch)\n","  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting segmentation-models-pytorch>=0.3 (from terratorch)\n","  Downloading segmentation_models_pytorch-0.4.0-py3-none-any.whl.metadata (32 kB)\n","Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from albucore<=0.0.16->terratorch) (1.26.4)\n","Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albucore<=0.0.16->terratorch) (4.10.0.84)\n","Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations<=1.4.10,>=1.3.1->terratorch) (1.13.1)\n","Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations<=1.4.10,>=1.3.1->terratorch) (0.25.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations<=1.4.10,>=1.3.1->terratorch) (6.0.2)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations<=1.4.10,>=1.3.1->terratorch) (4.12.2)\n","Collecting scikit-learn>=1.3.2 (from albumentations<=1.4.10,>=1.3.1->terratorch)\n","  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations<=1.4.10,>=1.3.1->terratorch) (2.10.4)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14.4->terratorch) (0.10.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14.4->terratorch) (23.2)\n","Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14.4->terratorch) (2.2.2)\n","Requirement already satisfied: pyproj>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14.4->terratorch) (3.7.0)\n","Requirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.14.4->terratorch) (2.0.6)\n","Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (2024.12.14)\n","Requirement already satisfied: hydra-core>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (1.3.2)\n","Collecting lightly_utils~=0.0.0 (from lightly>=1.4.25->terratorch)\n","  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (2.8.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (2.32.3)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (1.17.0)\n","Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (4.67.1)\n","Requirement already satisfied: pytorch_lightning>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (2.5.0.post0)\n","Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from lightly>=1.4.25->terratorch) (2.2.3)\n","Collecting aenum>=3.1.11 (from lightly>=1.4.25->terratorch)\n","  Downloading aenum-3.1.15-py3-none-any.whl.metadata (3.7 kB)\n","Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (2024.9.0)\n","Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (0.11.9)\n","Collecting jsonargparse<5.0,>=4.27.7 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->terratorch)\n","  Downloading jsonargparse-4.36.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: matplotlib<4.0,>3.1 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (3.7.5)\n","Requirement already satisfied: omegaconf<3.0,>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (2.3.0)\n","Requirement already satisfied: rich<14.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (13.9.4)\n","Collecting tensorboardX<3.0,>=2.2 (from lightning[pytorch-extra]!=2.3.*,>=2->terratorch)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Collecting bitsandbytes<1.0,>=0.44.0 (from lightning[pytorch-extra]!=2.3.*,>=2->terratorch)\n","  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\n","Collecting mlflow-skinny==2.20.1 (from mlflow>=2.12.1->terratorch)\n","  Downloading mlflow_skinny-2.20.1-py3-none-any.whl.metadata (31 kB)\n","Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.12.1->terratorch) (3.1.0)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.12.1->terratorch) (3.1.4)\n","Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.12.1->terratorch) (1.14.0)\n","Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.12.1->terratorch) (7.1.0)\n","Collecting graphene<4 (from mlflow>=2.12.1->terratorch)\n","  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n","Collecting gunicorn<24 (from mlflow>=2.12.1->terratorch)\n","  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.12.1->terratorch) (3.7)\n","Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.12.1->terratorch) (17.0.0)\n","Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow>=2.12.1->terratorch) (2.0.36)\n","Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (8.1.7)\n","Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (3.1.0)\n","Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch)\n","  Downloading databricks_sdk-0.43.0-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (3.1.43)\n","Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (7.2.1)\n","Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (1.29.0)\n","Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (1.29.0)\n","Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (3.20.3)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (0.5.3)\n","Requirement already satisfied: affine in /usr/local/lib/python3.10/dist-packages (from rasterio>=1.3.9->terratorch) (2.4.0)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio>=1.3.9->terratorch) (24.3.0)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio>=1.3.9->terratorch) (0.7.2)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio>=1.3.9->terratorch) (1.1.1)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from rasterio>=1.3.9->terratorch) (3.2.0)\n","Requirement already satisfied: xarray>=2024.7.0 in /usr/local/lib/python3.10/dist-packages (from rioxarray>=0.15.0->terratorch) (2024.11.0)\n","Collecting efficientnet-pytorch>=0.6.1 (from segmentation-models-pytorch>=0.3->terratorch)\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch>=0.3->terratorch) (0.27.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch>=0.3->terratorch) (10.4.0)\n","Collecting pretrainedmodels>=0.7.1 (from segmentation-models-pytorch>=0.3->terratorch)\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting timm>=0.9 (from segmentation-models-pytorch>=0.3->terratorch)\n","  Downloading timm-1.0.14-py3-none-any.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.5.0,>=2.1.0->terratorch) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<=2.5.0,>=2.1.0->terratorch) (3.4.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.21.5 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.4.127 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting triton==3.1.0 (from torch<=2.5.0,>=2.1.0->terratorch)\n","  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<=2.5.0,>=2.1.0->terratorch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<=2.5.0,>=2.1.0->terratorch) (1.3.0)\n","Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from torchgeo>=0.6.0->terratorch) (0.8.0)\n","Requirement already satisfied: fiona>=1.8.21 in /usr/local/lib/python3.10/dist-packages (from torchgeo>=0.6.0->terratorch) (1.10.1)\n","Requirement already satisfied: kornia>=0.7.3 in /usr/local/lib/python3.10/dist-packages (from torchgeo>=0.6.0->terratorch) (0.7.4)\n","Requirement already satisfied: rtree>=1 in /usr/local/lib/python3.10/dist-packages (from torchgeo>=0.6.0->terratorch) (1.3.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision>=0.16.0 (from terratorch)\n","  Downloading torchvision-0.21.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n","  Downloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n","  Downloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow>=2.12.1->terratorch) (1.3.8)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.12.1->terratorch) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.12.1->terratorch) (2.2.0)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow>=2.12.1->terratorch) (1.9.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (3.11.10)\n","Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.12.1->terratorch)\n","  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n","Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.12.1->terratorch)\n","  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.0.0->lightly>=1.4.25->terratorch) (4.9.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow>=2.12.1->terratorch) (3.0.2)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (0.16)\n","Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->terratorch)\n","  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: kornia-rs>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from kornia>=0.7.3->torchgeo>=0.6.0->terratorch) (0.1.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (75.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4.0,>3.1->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (1.4.7)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->albucore<=0.0.16->terratorch) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->albucore<=0.0.16->terratorch) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->albucore<=0.0.16->terratorch) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->albucore<=0.0.16->terratorch) (2025.0.1)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->albucore<=0.0.16->terratorch) (2022.0.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.24->albucore<=0.0.16->terratorch) (2.4.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas>=0.14.4->terratorch) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas>=0.14.4->terratorch) (2024.2)\n","Collecting munch (from pretrainedmodels>=0.7.1->segmentation-models-pytorch>=0.3->terratorch)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations<=1.4.10,>=1.3.1->terratorch) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations<=1.4.10,>=1.3.1->terratorch) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->lightly>=1.4.25->terratorch) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->lightly>=1.4.25->terratorch) (3.10)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (2.18.0)\n","Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations<=1.4.10,>=1.3.1->terratorch) (2.36.1)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations<=1.4.10,>=1.3.1->terratorch) (2024.12.12)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations<=1.4.10,>=1.3.1->terratorch) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations<=1.4.10,>=1.3.1->terratorch) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.2->albumentations<=1.4.10,>=1.3.1->terratorch) (3.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.12.1->terratorch) (3.1.1)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm>=0.9->segmentation-models-pytorch>=0.3->terratorch) (0.4.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (1.3.2)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (4.0.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,>=2->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (1.18.3)\n","Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (2.27.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (4.0.11)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (3.21.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (0.1.2)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (1.2.15)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (0.50b0)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,>=2->terratorch) (6.4.5)\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->albucore<=0.0.16->terratorch) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.24->albucore<=0.0.16->terratorch) (2022.0.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.24->albucore<=0.0.16->terratorch) (1.2.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.24->albucore<=0.0.16->terratorch) (2024.2.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (1.17.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (5.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (4.9)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.24->albucore<=0.0.16->terratorch) (2024.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.20.1->mlflow>=2.12.1->terratorch) (0.6.1)\n","Downloading terratorch-0.99.7-py3-none-any.whl (288 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.0/289.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading albucore-0.0.16-py3-none-any.whl (9.5 kB)\n","Downloading albumentations-1.4.10-py3-none-any.whl (161 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.9/161.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightly-1.5.18-py3-none-any.whl (849 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.0/849.0 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mlflow-2.20.1-py3-none-any.whl (28.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.3/28.3 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading mlflow_skinny-2.20.1-py3-none-any.whl (6.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading segmentation_models_pytorch-0.4.0-py3-none-any.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.5.0-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading torchgeo-0.6.2-py3-none-any.whl (454 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.7/454.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.20.0-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading aenum-3.1.15-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonargparse-4.36.0-py3-none-any.whl (214 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.5/214.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n","Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading timm-1.0.14-py3-none-any.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading databricks_sdk-0.43.0-py3-none-any.whl (647 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.4/647.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n","Downloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16424 sha256=4d55cf71f81ec47128bedd461a41f674081a8eb18d8a916dfb3de8d2589c9344\n","  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60944 sha256=2d9e85650d2ca02e91b4578778c0b7cedd41fc587b3db28605ba59deb9ee597a\n","  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n","Successfully built efficientnet-pytorch pretrainedmodels\n","Installing collected packages: aenum, typeshed-client, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, jsonargparse, gunicorn, graphql-core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, graphql-relay, nvidia-cusolver-cu12, graphene, databricks-sdk, torch, mlflow-skinny, efficientnet-pytorch, torchmetrics, torchvision, timm, tensorboardX, pretrainedmodels, lightning, lightly_utils, bitsandbytes, segmentation-models-pytorch, scikit-learn, lightly, albucore, torchgeo, mlflow, geopandas, albumentations, terratorch\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n","    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.23.4\n","    Uninstalling nvidia-nccl-cu12-2.23.4:\n","      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.7.77\n","    Uninstalling nvidia-curand-cu12-10.3.7.77:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n","    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n","      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n","    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n","    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n","    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n","      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n","    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n","    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n","    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu121\n","    Uninstalling torch-2.5.1+cu121:\n","      Successfully uninstalled torch-2.5.1+cu121\n","  Attempting uninstall: torchmetrics\n","    Found existing installation: torchmetrics 1.6.1\n","    Uninstalling torchmetrics-1.6.1:\n","      Successfully uninstalled torchmetrics-1.6.1\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.20.1+cu121\n","    Uninstalling torchvision-0.20.1+cu121:\n","      Successfully uninstalled torchvision-0.20.1+cu121\n","  Attempting uninstall: timm\n","    Found existing installation: timm 0.4.12\n","    Uninstalling timm-0.4.12:\n","      Successfully uninstalled timm-0.4.12\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","  Attempting uninstall: albucore\n","    Found existing installation: albucore 0.0.19\n","    Uninstalling albucore-0.0.19:\n","      Successfully uninstalled albucore-0.0.19\n","  Attempting uninstall: geopandas\n","    Found existing installation: geopandas 0.14.1\n","    Uninstalling geopandas-0.14.1:\n","      Successfully uninstalled geopandas-0.14.1\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 1.4.20\n","    Uninstalling albumentations-1.4.20:\n","      Successfully uninstalled albumentations-1.4.20\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\n","pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 24.12.0 which is incompatible.\n","pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 24.12.1 which is incompatible.\n","torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed aenum-3.1.15 albucore-0.0.16 albumentations-1.4.10 bitsandbytes-0.45.1 databricks-sdk-0.43.0 efficientnet-pytorch-0.7.1 geopandas-1.0.1 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 jsonargparse-4.36.0 lightly-1.5.18 lightly_utils-0.0.2 lightning-2.5.0.post0 mlflow-2.20.1 mlflow-skinny-2.20.1 munch-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 pretrainedmodels-0.7.4 scikit-learn-1.6.1 segmentation-models-pytorch-0.4.0 tensorboardX-2.6.2.2 terratorch-0.99.7 timm-1.0.14 torch-2.5.0 torchgeo-0.6.2 torchmetrics-1.3.1 torchvision-0.20.0 triton-3.1.0 typeshed-client-2.7.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install terratorch"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:46:26.651980Z","iopub.status.busy":"2025-02-05T11:46:26.651554Z","iopub.status.idle":"2025-02-05T11:46:34.631250Z","shell.execute_reply":"2025-02-05T11:46:34.630363Z","shell.execute_reply.started":"2025-02-05T11:46:26.651944Z"},"trusted":true},"outputs":[],"source":["## EXECUTE\n","import torch\n","import torch.nn as nn\n","import lightning.pytorch as pl\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","from terratorch.tasks import SemanticSegmentationTask\n","from einops import rearrange\n","\n","class PrithviSegmentation(pl.LightningModule):\n","    def __init__(self, num_classes: int = 2, in_channels: int = 6, learning_rate: float = 1e-4):\n","        \"\"\"Initialize the segmentation model using Prithvi 2.0 backbone.\"\"\"\n","        super().__init__()\n","        self.learning_rate = learning_rate\n","\n","        # Load Prithvi-EO-2.0-300M as a backbone\n","        self.model = SemanticSegmentationTask(\n","            model_args={\n","                \"decoder\": \"UperNetDecoder\",\n","                \"backbone_pretrained\": True,\n","                \"backbone\": \"prithvi_eo_v2_600_tl\",  \n","                \"backbone_in_channels\": in_channels,  \n","                \"rescale\": True,\n","                \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_BROAD\", \"SWIR_1\", \"SWIR_2\"],\n","                \"backbone_num_frames\": 1,\n","                \"num_classes\": num_classes,\n","                \"head_dropout\": 0.1,\n","                \"decoder_channels\": 256,\n","                \"decoder_scale_modules\": True,\n","                \"head_channel_list\": [128, 64],\n","                \"necks\": [\n","                    {\"name\": \"SelectIndices\", \"indices\": [7, 15, 23, 31]},\n","                    {\"name\": \"ReshapeTokensToImage\"},\n","                ],\n","            },\n","            plot_on_val=False,\n","            loss=\"focal\",\n","            lr=self.learning_rate,\n","            optimizer=\"AdamW\",\n","            optimizer_hparams={\"weight_decay\": 0.1},\n","            scheduler=\"StepLR\",\n","            scheduler_hparams={\"step_size\": 10, \"gamma\": 0.9},\n","            ignore_index=-1,\n","            freeze_backbone=False,\n","            freeze_decoder=False,\n","            model_factory=\"EncoderDecoderFactory\",\n","        )\n","\n","    \n"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T11:46:34.633221Z","iopub.status.busy":"2025-02-05T11:46:34.632468Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('ibm-nasa-geospatial/Prithvi-EO-2.0-300M-TL', 'Prithvi_EO_V2_300M_TL.pt'))\n"]}],"source":["## EXECUTE\n","# Import PyTorch Lightning Trainer\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","from lightning.pytorch.loggers import TensorBoardLogger\n","\n","from terratorch.tasks import SemanticSegmentationTask\n","# Logger\n","logger = TensorBoardLogger(save_dir=\"desert_locust\", name=\"segmentation\")\n","\n","# Checkpoint Callback\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath=\"checkpoints/\",\n","    filename=\"epoch-{epoch:02d}\",\n","    save_top_k=-1,\n","    every_n_epochs=1,\n","    save_on_train_epoch_end=True\n",")\n","\n","# Setup Trainer\n","model = SemanticSegmentationTask(\n","            model_args={\n","        \"decoder\": \"UperNetDecoder\",\n","        \"backbone_pretrained\": True,\n","        \"backbone\": \"prithvi_eo_v2_300_tl\", # Model can be either prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n","        \"backbone_in_channels\": 6,\n","        \"backbone_features_only\": True,\n","        # \"backbone_coords_encoding\": [\"time\", \"location\"], # this must be used to use time and location metadata\n","        \"rescale\": True,\n","        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n","        \"backbone_num_frames\": 3,\n","        \"num_classes\": 2,\n","        \"head_dropout\": 0.1,\n","        \"decoder_channels\": 256,\n","        \"decoder_scale_modules\": True,\n","        \"necks\": [\n","            {\n","                \"name\": \"SelectIndices\",\n","                #\"indices\": [2, 5, 8, 11] # indices for prithvi_vit_100\n","                \"indices\": [5, 11, 17, 23] # indices for prithvi_eo_v2_300\n","                #\"indices\": [7, 15, 23, 31] # indices for prithvi_eo_v2_600\n","            },\n","            {\n","                \"name\": \"ReshapeTokensToImage\",\n","                \"effective_time_dim\": 3\n","            }\n","        ]\n","            },\n","            plot_on_val=False,\n","            loss=\"ce\",\n","            lr=1.0e-4,\n","            optimizer=\"AdamW\",\n","            optimizer_hparams={\"weight_decay\": 0.1},\n","            ignore_index=-1,\n","            freeze_backbone=True,\n","            freeze_decoder=False,\n","            model_factory=\"EncoderDecoderFactory\",\n","        )\n","trainer = pl.Trainer(\n","    accelerator=\"auto\",\n","    strategy=\"auto\",\n","    devices=\"auto\",\n","    num_nodes=1,\n","    logger = logger,\n","    max_epochs=50,\n","    check_val_every_n_epoch=1,\n","    log_every_n_steps=1,\n","    enable_checkpointing=True,\n","    callbacks=[checkpoint_callback],\n","    default_root_dir=\"root_dir\",\n",")\n","\n","# Train Model\n","trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader) #, ckpt_path=\"/kaggle/working/epoch-epoch=00.ckpt\" )"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:36:48.490781Z","iopub.status.busy":"2025-02-05T09:36:48.490476Z","iopub.status.idle":"2025-02-05T09:36:48.513068Z","shell.execute_reply":"2025-02-05T09:36:48.512193Z","shell.execute_reply.started":"2025-02-05T09:36:48.490757Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader, Dataset\n","import numpy as np\n","import rasterio\n","import os\n","import torch\n","\n","class TestDataset(Dataset):\n","    \"\"\"Dataset class for test images (without labels).\"\"\"\n","    def __init__(self, test_image_paths):\n","        self.test_image_paths = test_image_paths\n","\n","    def __len__(self):\n","        return len(self.test_image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.test_image_paths[idx]\n","        with rasterio.open(image_path) as src:\n","            image = src.read()  # Read image as numpy array (C, H, W)\n","        image = torch.tensor(image, dtype=torch.float32) \n","        image = image.view(6,3,256,256)\n","        return image, image_path  # Return image and filename\n","\n","# Load test image paths\n","test_images_dir = \"extracted_files/hls_test/chips\"\n","test_image_paths = [os.path.join(test_images_dir, img) for img in os.listdir(test_images_dir)]\n","\n","# Create test dataloader\n","test_dataset = TestDataset(test_image_paths)\n","test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:36:51.736671Z","iopub.status.busy":"2025-02-05T09:36:51.736372Z","iopub.status.idle":"2025-02-05T09:36:51.756625Z","shell.execute_reply":"2025-02-05T09:36:51.755732Z","shell.execute_reply.started":"2025-02-05T09:36:51.736646Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 6, 3, 256, 256])\n"]}],"source":["for batch in test_loader:\n","    print(batch[0].shape)\n","    break"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:36:55.124333Z","iopub.status.busy":"2025-02-05T09:36:55.123985Z","iopub.status.idle":"2025-02-05T09:37:02.521640Z","shell.execute_reply":"2025-02-05T09:37:02.520645Z","shell.execute_reply.started":"2025-02-05T09:36:55.124296Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (('ibm-nasa-geospatial/Prithvi-EO-2.0-300M-TL', 'Prithvi_EO_V2_300M_TL.pt'))\n"]},{"data":{"text/plain":["SemanticSegmentationTask(\n","  (model): PixelWiseModel(\n","    (encoder): TimmBackboneWrapper(\n","      (_timm_module): PrithviViT(\n","        (patch_embed): PatchEmbed(\n","          (proj): Conv3d(6, 1024, kernel_size=(1, 16, 16), stride=(1, 16, 16))\n","          (norm): Identity()\n","        )\n","        (temporal_embed_enc): TemporalEncoder()\n","        (location_embed_enc): LocationEncoder()\n","        (blocks): ModuleList(\n","          (0-23): 24 x Block(\n","            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (attn): Attention(\n","              (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n","              (q_norm): Identity()\n","              (k_norm): Identity()\n","              (attn_drop): Dropout(p=0.0, inplace=False)\n","              (proj): Linear(in_features=1024, out_features=1024, bias=True)\n","              (proj_drop): Dropout(p=0.0, inplace=False)\n","            )\n","            (ls1): Identity()\n","            (drop_path1): Identity()\n","            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Mlp(\n","              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n","              (act): GELU(approximate='none')\n","              (drop1): Dropout(p=0.0, inplace=False)\n","              (norm): Identity()\n","              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n","              (drop2): Dropout(p=0.0, inplace=False)\n","            )\n","            (ls2): Identity()\n","            (drop_path2): Identity()\n","          )\n","        )\n","        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      )\n","    )\n","    (decoder): UperNetDecoder(\n","      (fpn1): Sequential(\n","        (0): ConvTranspose2d(3072, 1536, kernel_size=(2, 2), stride=(2, 2))\n","        (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): GELU(approximate='none')\n","        (3): ConvTranspose2d(1536, 768, kernel_size=(2, 2), stride=(2, 2))\n","      )\n","      (fpn2): Sequential(\n","        (0): ConvTranspose2d(3072, 1536, kernel_size=(2, 2), stride=(2, 2))\n","      )\n","      (fpn3): Sequential(\n","        (0): Identity()\n","      )\n","      (fpn4): Sequential(\n","        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","      )\n","      (psp_modules): PPM(\n","        (0): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=1)\n","          (1): ConvModule(\n","            (conv): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (act): ReLU(inplace=True)\n","          )\n","        )\n","        (1): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=2)\n","          (1): ConvModule(\n","            (conv): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (act): ReLU(inplace=True)\n","          )\n","        )\n","        (2): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=3)\n","          (1): ConvModule(\n","            (conv): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (act): ReLU(inplace=True)\n","          )\n","        )\n","        (3): Sequential(\n","          (0): AdaptiveAvgPool2d(output_size=6)\n","          (1): ConvModule(\n","            (conv): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (act): ReLU(inplace=True)\n","          )\n","        )\n","      )\n","      (bottleneck): ConvModule(\n","        (conv): Conv2d(4096, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act): ReLU(inplace=True)\n","      )\n","      (lateral_convs): ModuleList(\n","        (0): ConvModule(\n","          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (act): ReLU()\n","        )\n","        (1): ConvModule(\n","          (conv): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (act): ReLU()\n","        )\n","        (2): ConvModule(\n","          (conv): Conv2d(3072, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (act): ReLU()\n","        )\n","      )\n","      (fpn_convs): ModuleList(\n","        (0-2): 3 x ConvModule(\n","          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (act): ReLU()\n","        )\n","      )\n","      (fpn_bottleneck): ConvModule(\n","        (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (act): ReLU(inplace=True)\n","      )\n","    )\n","    (head): SegmentationHead(\n","      (head): Sequential(\n","        (0): Identity()\n","        (1): Dropout(p=0.1, inplace=False)\n","        (2): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","    )\n","    (aux_heads): ModuleDict()\n","    (neck): Sequential(\n","      (0): SelectIndices()\n","      (1): ReshapeTokensToImage()\n","    )\n","  )\n","  (criterion): CrossEntropyLoss()\n","  (train_metrics): MetricCollection(\n","    (Multiclass_Accuracy): MulticlassAccuracy()\n","    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n","      (metric): MulticlassAccuracy()\n","    )\n","    (Multiclass_F1_Score): MulticlassF1Score()\n","    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n","    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n","      (metric): MulticlassJaccardIndex()\n","    )\n","    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n","    prefix=train/\n","  )\n","  (val_metrics): MetricCollection(\n","    (Multiclass_Accuracy): MulticlassAccuracy()\n","    (Multiclass_Accuracy_Class): ClasswiseWrapper(\n","      (metric): MulticlassAccuracy()\n","    )\n","    (Multiclass_F1_Score): MulticlassF1Score()\n","    (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n","    (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n","      (metric): MulticlassJaccardIndex()\n","    )\n","    (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n","    prefix=val/\n","  )\n","  (test_metrics): ModuleList(\n","    (0): MetricCollection(\n","      (Multiclass_Accuracy): MulticlassAccuracy()\n","      (Multiclass_Accuracy_Class): ClasswiseWrapper(\n","        (metric): MulticlassAccuracy()\n","      )\n","      (Multiclass_F1_Score): MulticlassF1Score()\n","      (Multiclass_Jaccard_Index): MulticlassJaccardIndex()\n","      (Multiclass_Jaccard_Index_Class): ClasswiseWrapper(\n","        (metric): MulticlassJaccardIndex()\n","      )\n","      (Multiclass_Jaccard_Index_Micro): MulticlassJaccardIndex(),\n","      prefix=test/\n","    )\n","  )\n",")"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import os\n","import rasterio\n","import numpy as np\n","import pandas as pd\n","from pyproj import Transformer\n","from rasterio.transform import AffineTransformer\n","from lightning.pytorch.callbacks import ModelCheckpoint\n","from lightning.pytorch.loggers import TensorBoardLogger\n","from terratorch.tasks import SemanticSegmentationTask\n","\n","# Define the checkpoint path\n","best_ckpt_path = \"epoch-epoch=01.ckpt\"  # Replace XX with the best epoch\n","\n","# Load the model from checkpoint\n","model = SemanticSegmentationTask.load_from_checkpoint(\n","    best_ckpt_path,\n","    model_args=model.hparams.model_args,\n","    model_factory=model.hparams.model_factory\n",")\n","\n","# Move model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 6, 3, 256, 256])\n","('extracted_files/hls_test/chips/chip_20210101_S30_T36QXD_2020338T081319_12_0.tif',)\n"]}],"source":["for image, filename in test_loader:\n","    print(image.shape)\n","    print(filename)\n","    break"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:37:23.371851Z","iopub.status.busy":"2025-02-05T09:37:23.371550Z","iopub.status.idle":"2025-02-05T09:54:08.245009Z","shell.execute_reply":"2025-02-05T09:54:08.244177Z","shell.execute_reply.started":"2025-02-05T09:37:23.371828Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2404/2404 [1:03:07<00:00,  1.58s/it]\n"]}],"source":["import numpy as np\n","from tqdm import tqdm\n","predictions_dir = \"predictions\"\n","os.makedirs(predictions_dir, exist_ok=True)\n","\n","with torch.no_grad():\n","    for image, filename in tqdm(test_loader):\n","        image = image.to(device)\n","        output = model(image)\n","        pred_mask = torch.argmax(output.output, dim=1).cpu().numpy()[0]  # Get predicted mask\n","\n","        # Save prediction as a raster\n","        save_path = os.path.join(predictions_dir, os.path.basename(filename[0]).replace(\".tif\", \"_pred.tif\"))\n","        with rasterio.open(filename[0]) as src:\n","            meta = src.meta.copy()\n","            meta.update(dtype=rasterio.int16, nodata=-9999)\n","            with rasterio.open(save_path, \"w\", **meta) as dst:\n","                dst.write(pred_mask.astype(np.uint8), 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:55:06.683712Z","iopub.status.busy":"2025-02-05T09:55:06.683403Z","iopub.status.idle":"2025-02-05T09:55:06.691341Z","shell.execute_reply":"2025-02-05T09:55:06.690437Z","shell.execute_reply.started":"2025-02-05T09:55:06.683688Z"},"trusted":true},"outputs":[],"source":["print(len(test_loader))\n","print(len(os.listdir(\"/kaggle/working/predictions_s2\")))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:55:11.758011Z","iopub.status.busy":"2025-02-05T09:55:11.757699Z","iopub.status.idle":"2025-02-05T09:58:11.667117Z","shell.execute_reply":"2025-02-05T09:58:11.666187Z","shell.execute_reply.started":"2025-02-05T09:55:11.757983Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","from rasterio.transform import AffineTransformer\n","from pyproj import Transformer\n","from rasterio.crs import CRS\n","\n","# Read test CSV\n","submission_df = pd.read_csv(\"/kaggle/input/geo-ai-hack/test.csv\")\n","\n","def get_prediction_value(row):\n","    \"\"\"Find the predicted value for each test location.\"\"\"\n","    matching_files = [f for f in os.listdir(predictions_dir) if (str(row['date']) in f) and (row['mgrs_tile_id'] in f)]\n","    if not matching_files:\n","        return np.nan\n","\n","    for file in matching_files:\n","        with rasterio.open(os.path.join(predictions_dir, file)) as src:\n","            width, height = src.width, src.height\n","            affine_transform = AffineTransformer(src.transform)\n","            transformer = Transformer.from_crs(CRS.from_epsg(4326), src.crs, always_xy=True)\n","            x_chip, y_chip = transformer.transform(row['x'], row['y'])\n","            x_offset, y_offset = affine_transform.rowcol(x_chip, y_chip)\n","\n","            if 0 <= x_offset < width and 0 <= y_offset < height:\n","                return src.read(1)[y_offset, x_offset]\n","\n","    return np.nan\n","\n","# Apply function to get predictions\n","submission_df[\"prediction\"] = submission_df.apply(get_prediction_value, axis=1)\n","\n","# Save final submission\n","submission_df[[\"id\", \"prediction\"]].to_csv(\"s2_submission.csv\", index=False)\n","\n","print(\"✅ Submission file saved as `hls_submission.csv`\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:58:35.596471Z","iopub.status.busy":"2025-02-05T09:58:35.596102Z","iopub.status.idle":"2025-02-05T09:58:35.604001Z","shell.execute_reply":"2025-02-05T09:58:35.602979Z","shell.execute_reply.started":"2025-02-05T09:58:35.596440Z"},"trusted":true},"outputs":[],"source":["submission_df['prediction'].unique()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2025-02-05T09:33:28.024329Z","iopub.status.busy":"2025-02-05T09:33:28.023986Z","iopub.status.idle":"2025-02-05T09:33:28.034048Z","shell.execute_reply":"2025-02-05T09:33:28.033344Z","shell.execute_reply.started":"2025-02-05T09:33:28.024300Z"},"trusted":true},"outputs":[],"source":["submission_df[[\"id\", \"prediction\"]].to_csv(\"s2_submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":11007828,"sourceId":91683,"sourceType":"competition"},{"isSourceIdPinned":true,"modelId":235926,"modelInstanceId":214253,"sourceId":250655,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30840,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
